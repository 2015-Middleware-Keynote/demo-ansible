# vim: set ft=ansible:
---
- name: Create group for infra hosts in this env only
  hosts: localhost
  gather_facts: no
  connection: local
  sudo: no
  vars:
    infra_hosts: "{{ groups['tag_node-region_infra'] | intersect(groups['tag_env_' ~ cluster_id]) }}"
  tasks:
  - add_host:
      name: "{{ infra_hosts.0 }}"
      groups: oo_first_infra_node

- name: Verify hosts in the infra region have core ansible facts
  hosts: oo_first_infra_node

- name: Node post config
  hosts: oo_nodes_to_config
  vars_files:
  - ../vars.yml
  tasks:
  - lineinfile:
      dest: /etc/sysconfig/docker
      regexp: "{{ item.regexp }}"
      line: "{{ item.line }}"
    with_items:
    - regexp: INSECURE_REGISTRY=
      line: INSECURE_REGISTRY='--insecure-registry 0.0.0.0/0'
    - regexp: GOMAXPROCS=
      line: GOMAXPROCS={{ ansible_processor_cores }}
    register: docker_conf

  - name: restart docker
    service:
      name: docker
      state: restarted
    when: docker_conf | changed

  - lineinfile:
      dest: /etc/resolv.conf
      insertbefore: ^nameserver 172.*$
      line: nameserver {{ hostvars[groups.oo_first_master.0].openshift.dns.ip }}
      state: present

  - lineinfile:
      dest: /etc/dhcp/dhclient-eth0.conf
      line: prepend domain-name-servers {{ hostvars[groups.oo_first_master.0].openshift.dns.ip }}
      state: present
      create: yes

  # Temporary workaround until openshift-ansible supports setting
  # kubeletArguments directly
  - name: Set kubeletArguments
    lineinfile:
      dest: /etc/openshift/node/node-config.yaml
      state: present
      regexp: '^kubeletArguments:'
      line: 'kubeletArguments: { max-pods: ["100"] }'
    notify: restart openshift-node

  - name: pre-pull images
    command: "docker pull {{ item }}"
    with_items: preload_images
  handlers:
    - name: restart openshift-node
      service: name=openshift-node state=restarted

- name: First master post config
  hosts: oo_first_master
  vars_files:
  - ../vars.yml
  vars:
    proxy:
      app_name: sketch
      name: sketchproxy
      project: default
      region: demo
    users:
    - name: demo
      project: demo
    - name: hexboard
      project: hexboard
    - name: "{{ prime_user }}"
      project: "{{ prime_namespace }}"
    default_context: '{{ "default/" ~ (hostvars[groups.oo_first_master.0].openshift.common.hostname | regex_replace("\.", "-")) ~ ":" ~ hostvars[groups.oo_first_master.0].openshift.master.api_port ~ "/system:admin" }}'
  tasks:
  - name: Create the default users
    command: "htpasswd -b /etc/openshift/master/htpasswd {{ item.name }} {{ default_password }}"
    with_items: users

  - name: Create the 40 workshop users
    command: "htpasswd -b /etc/openshift/master/htpasswd {{ item }} {{ default_password }}"
    with_sequence: start=0 end=40 format=user%02x

  - command: oc project default

  - name: Fix node labels
    command: oc label --overwrite nodes {{ hostvars[item].openshift.common.hostname }} region={{ hostvars[item].openshift_node_labels.region }} zone={{ hostvars[item].openshift_node_labels.zone }}
    with_items: groups.oo_nodes_to_config

  - name: Unschedule master
    command: oadm manage-node --selector="region=master" --schedulable=false

  - name: Set nodeselector for the default project to be region=infra
    shell: "oc get namespace default -o yaml  | sed -e '/  annotations:/a\\    openshift.io/node-selector: region=infra' | oc replace -f -"

  - name: Verify whether a router exists or not
    command: oadm router --dry-run --service-account=router
    register: router_out
    ignore_errors: true

  - name: Create router service account
    shell: echo '{"kind":"ServiceAccount","apiVersion":"v1","metadata":{"name":"router"}}' | oc create -f -
    when: router_out | failed

  - name: Add router service account to privileged SCC
    shell: "oc get scc privileged -o yaml | sed '/.*:build-controller/a - system:serviceaccount:default:router' | oc replace -f -"
    when: router_out | failed

  - name: Create router cert temporary directory
    file:
      dest: "~{{ ansible_ssh_user }}/router_certs"
      state: directory
    when: router_out | failed

  - name: Generate router certificate files
    command: "oadm create-server-cert --signer-cert=/etc/openshift/master/ca.crt --signer-key=/etc/openshift/master/ca.key --signer-serial=/etc/openshift/master/ca.serial.txt --hostnames='*.{{ r53_wildcard_zone }}' --cert={{ r53_wildcard_zone }}.crt --key={{ r53_wildcard_zone }}.key"
    when: router_out | failed

  - name: Assemble router PEM
    assemble:
      dest: "~{{ ansible_ssh_user }}/{{ r53_wildcard_zone }}.pem"
      src: "~{{ ansible_ssh_user }}/router_certs"
    when: router_out | failed

  - name: Install router
    command: "oadm router --default-cert={{ r53_wildcard_zone }}.pem --credentials=/etc/openshift/master/openshift-router.kubeconfig --service-account=router"
    when: router_out | failed

  - name: Check whether a registry exists or not
    command: oadm registry --dry-run
    register: registry_out
    ignore_errors: true

  - name: Install registry
    command: "oadm registry --credentials=/etc/openshift/master/openshift-registry.kubeconfig --images='{{ registry_image_url }}'"
    when: registry_out | failed

  - command: oc env dc/docker-registry GOMAXPROCS={{ hostvars[groups.oo_first_infra_node.0].ansible_processor_cores }}

  - pause: minutes=1

  - command: oc get projects
    register: projects

  - command: "oadm new-project {{ item.project }} --display-name='{{ item.project.title() }}' --node-selector='region={{ item.project }}'"
    when: item.project not in projects.stdout
    with_items: users

  - command: oc project default

  - command: oadm policy add-role-to-user admin {{ item.name }} -n {{ item.project }}
    when: item.project not in projects.stdout
    with_items: users

  - command: oc project default

  - command: oc get dc -n {{ hexboard.namespace }}
    register: dcs_out
    failed_when: "hexboard.name not in dcs_out"
    ignore_errors: true

  - command: oc login -u {{ users.0.name }} -p {{ default_password }}
    when: dcs_out | failed

  - script: ../files/get_token.sh
    register: auth_token
    when: dcs_out | failed

  - set_fact:
      access_token: "{{ auth_token.stdout }}"
    when: dcs_out | failed

  - command: oc project {{ hexboard.namespace }}
    when: dcs_out | failed

  - template:
      dest: /root/hexboard_template.json
      src: ../templates/hexboard_template.json.j2
    when: dcs_out | failed

  - command: oc create -f /root/hexboard_template.json
    when: dcs_out | failed
    ignore_errors: true

  # TODO: it appears the build fails sometimes, need to do better validation
  # on the build and attempt 1 or more retries if it fails
  - command: oc start-build {{ hexboard.name }}
    when: dcs_out | failed

  - command: "oc config use-context {{ default_context }}"
    when: dcs_out | failed

  # Set up the smoke project and app
  - name: Create user smoke test projects
    command: "oadm new-project {{ item }}-smoke --display-name='Smoke Test' --admin={{ item }}"
    when: item not in projects.stdout
    with_sequence: start=0 end=40 format=user%02d

  - name: Create "app" in smoke projects
    command: "oc new-app openshift/php~https://github.com/gshipley/smoke.git -n {{ item }}-smoke"
    when: item not in projects.stdout
    with_sequence: start=0 end=40 format=user%02d

  - name: Expose smoke project service
    command: "oc expose service smoke -n {{ item }}-smoke"
    when: item not in projects.stdout
    with_sequence: start=0 end=40 format=user%02d

  - name: Wait to scale smoke app for first deployment
    shell: "oc get rc --all-namespaces | grep smoke | wc -l"
    register: build_output
    until: build_output.stdout.find("41") != -1
    retries: 11
    delay: 30

  - name: Scale smoke app
    command: "oc scale dc/smoke --replicas=2 -n {{ item }}-smoke"
    when: item not in projects.stdout
    with_sequence: start=0 end=40 format=user%02d

  - command: oc project default

  - command: oc get service
    register: service_out
  - set_fact:
      registry_ip: "{{ item.split().3 }}"
      registry_port: "{{ item.split().4.split('/').0 }}"
    with_items: service_out.stdout_lines
    when: "'docker-registry' in item"
  handlers:
    - name: restart openshift-master
      service: name=openshift-master state=restarted

- name: Node post config
  hosts: oo_nodes_to_config
  vars_files:
  - ../vars.yml
  vars:
    registry_ip: "{{ hostvars[groups.oo_first_master.0].registry_ip }}"
    registry_port: "{{ hostvars[groups.oo_first_master.0].registry_port }}"
    default_context: '{{ "default/" ~ (hostvars[groups.oo_first_master.0].openshift.common.hostname | regex_replace("\.", "-")) ~ ":" ~ hostvars[groups.oo_first_master.0].openshift.master.api_port ~ "/system:admin" }}'
  serial: 1
  tasks:
  - command: docker pull openshift3/{{ item.image }}:{{ item.tag }}
    with_items: prime_images

  - command: docker tag -f openshift3/{{item.image}} {{ registry_ip }}:{{ registry_port }}/{{ prime_namespace }}/{{ item.image }}:{{ item.tag }}
    with_items: prime_images

  - command: oc login --insecure-skip-tls-verify -u {{ prime_user }} -p {{ default_password }} {{ hostvars[groups.oo_first_master.0].openshift.master.api_url }}

  - command: oc whoami -t
    register: token_result

  - command: "oc config use-context {{ default_context }}"
    when: inventory_hostname == groups.oo_first_master.0

  - command: oc project default
    when: inventory_hostname == groups.oo_first_master.0

  - command: docker login -u {{ prime_user }} -e {{ prime_user }}@{{ prime_namespace }}.local -p {{ token_result.stdout }} {{ registry_ip }}:{{ registry_port }}

  - command: docker push {{ registry_ip }}:{{ registry_port }}/{{ prime_namespace }}/{{ item.image }}:{{ item.tag }}
    with_items: prime_images

  - command: docker logout

