# vim: set ft=ansible:
---
# to get the storage volume info and set up groups in case this playbook is run by itself
- include: cloudformation_setup.yml
- include: group_setup.yml

- name: Node post configuration
  hosts: nodes
  vars_files:
  - vars.yml
  tasks:
  - name: pre-pull images
    command: "docker pull {{ item }}"
    with_items: preload_images

- name: User creation
  hosts: masters
  vars:
    default_context: 'default/openshift-internal-{{ r53_host_zone  | regex_replace("\.", "-") }}:{{ api_port }}/system:admin'
  vars_files:
  - vars.yml
  tasks:
  - name: Change the oc context
    command: "oc config use-context {{ default_context }}"

  - name: Switch to default project
    command: oc project default

  - name: Create the default users
    command: "htpasswd -b /etc/origin/master/htpasswd {{ item.user }} {{ default_password }}"
    with_items: users
  
- name: Configure default project
  hosts: project_master
  vars_files:
  - vars.yml
  vars:
    default_context: 'default/openshift-internal-{{ r53_host_zone  | regex_replace("\.", "-") }}:{{ api_port }}/system:admin'
  tasks:
  - name: Change the oc context
    command: "oc config use-context {{ default_context }}"

  - name: Switch to default project
    command: oc project default

  - name: Set nodeselector for the default project to be env=infra
    shell: "oc get namespace default -o yaml  | sed -e '/  annotations:/a\\    openshift.io/node-selector: env=infra' | oc replace -f -"

- name: Installation and Configuration of Router
  hosts: project_master
  vars_files:
  - vars.yml
  vars:
    default_context: 'default/openshift-internal-{{ r53_host_zone  | regex_replace("\.", "-") }}:{{ api_port }}/system:admin'
  tasks:
  - name: Change the oc context
    command: "oc config use-context {{ default_context }}"

  - name: Switch to default project
    command: oc project default

  - name: Verify whether a router exists or not
    command: oadm router --dry-run --service-account=router
    register: router_out
    ignore_errors: true

  - name: Create router cert temporary directory
    file:
      dest: "~{{ ansible_ssh_user }}/router_certs"
      state: directory
    when: router_out | failed

  - name: Generate router certificate files
    command: "oadm create-server-cert --signer-cert=/etc/origin/master/ca.crt --signer-key=/etc/origin/master/ca.key --signer-serial=/etc/origin/master/ca.serial.txt --hostnames='*.{{ r53_wildcard_zone }}' --cert={{ r53_wildcard_zone }}.crt --key={{ r53_wildcard_zone }}.key"
    when: router_out | failed

  - name: Assemble router PEM
    assemble:
      dest: "~{{ ansible_ssh_user }}/{{ r53_wildcard_zone }}.pem"
      src: "~{{ ansible_ssh_user }}/router_certs"
    when: router_out | failed

  - name: Install router
    command: "oadm router --default-cert={{ r53_wildcard_zone }}.pem --credentials=/etc/origin/master/openshift-router.kubeconfig --service-account=router --images='{{ router_image_url }}'"
    when: router_out | failed

  # we should probably do some kind of actual check for router deployment
  - name: Wait for router to deploy
    pause:
      seconds: 10

  - name: Scale router
    command: "oc scale --replicas={{ num_infra_nodes }} dc router"
    when: router_out | failed

# Using EBS storage with OpenShift requires that the systems
# know the EC2 credentials in order to manipulate the EBS volumes.
# masters require the keys in /etc/sysconfig/atomic-openshift-master
- name: Set up master EC2 credentials
  hosts: masters
  gather_facts: no
  vars_files:
  - vars.yml
  tasks:
  - name: Write EC2 key ID to /etc/sysconfig/atomic-openshift-master
    lineinfile:
      dest: /etc/sysconfig/atomic-openshift-master
      insertafter: EOF
      line: "AWS_ACCESS_KEY_ID={{ lookup('env','AWS_ACCESS_KEY_ID') }}"
      regexp: '^AWS_ACCESS_KEY_ID=.*'
    register: master_id_result

  - name: Write EC2 secret key to /etc/sysconfig/atomic-openshift-master
    lineinfile:
      dest: /etc/sysconfig/atomic-openshift-master
      insertafter: EOF
      line: "AWS_SECRET_ACCESS_KEY={{ lookup('env','AWS_SECRET_ACCESS_KEY') }}"
      regexp: '^AWS_SECRET_ACCESS_KEY=.*'
    register: master_key_result

  - name: Restart atomic-openshift-master-controllers service
    service: 
      name: atomic-openshift-master-controllers
      state: restarted
    when: (master_id_result | changed) or (master_key_result | changed)

# Using EBS storage with OpenShift requires that the systems
# know the EC2 credentials in order to manipulate the EBS volumes.
# nodes require the keys in /etc/sysconfig/atomic-openshift-node
- name: Set up node EC2 credentials
  hosts: nodes
  gather_facts: no
  vars_files:
  - vars.yml
  tasks:
  - name: Write EC2 key ID to /etc/sysconfig/atomic-openshift-node
    lineinfile:
      dest: /etc/sysconfig/atomic-openshift-node
      insertafter: EOF
      line: "AWS_ACCESS_KEY_ID={{ lookup('env','AWS_ACCESS_KEY_ID') }}"
      regexp: '^AWS_ACCESS_KEY_ID=.*'
    register: node_id_result

  - name: Write EC2 secret key to /etc/sysconfig/atomic-openshift-node
    lineinfile:
      dest: /etc/sysconfig/atomic-openshift-node
      insertafter: EOF
      line: "AWS_SECRET_ACCESS_KEY={{ lookup('env','AWS_SECRET_ACCESS_KEY') }}"
      regexp: '^AWS_SECRET_ACCESS_KEY=.*'
    register: node_key_result

  - name: Restart atomic-openshift-node service
    service: 
      name: atomic-openshift-node
      state: restarted
    when: (node_id_result | changed) or (node_key_result | changed)

- name: Installation and Configuration of Registry
  hosts: project_master
  gather_facts: no
  vars_files:
  - vars.yml
  vars:
    default_context: 'default/openshift-internal-{{ r53_host_zone  | regex_replace("\.", "-") }}:{{ api_port }}/system:admin'
  tasks:

  # make sure that we are using the default user (system:admin) and the default project
  - name: Change the oc context
    command: "oc config use-context {{ default_context }}"
  
  - name: Switch to default project
    command: oc project default

  - name: Check whether a registry exists or not
    command: oadm registry --dry-run
    register: registry_out
    ignore_errors: true

  - name: Install registry
    command: "oadm registry --credentials=/etc/origin/master/openshift-registry.kubeconfig --images='{{ registry_image_url }}'"
    when: registry_out | failed

  # we use a template to then lay down YAML to create the PV
  # this sets facts that are then consumed in the template
  - name: Set the facts for the registry PV template
    set_fact:
      pv_name: "registry-pv"
      capacity: "50"
      volid: "{{ hostvars['localhost']['registry_volume'] }}"
      access_mode: "ReadWriteMany"
      
  - name: Create a YAML file for the PV for the Registry volume
    template:
      src: templates/pv.yaml.j2
      dest: /root/registry-pv.yaml

  - name: Check for registry PV
    command: oc get pv "{{ pv_name }}"
    register: registry_pv_out
    ignore_errors: true

  # as before
  - name: Set the facts for the registry PVC template
    set_fact:
      claim_name: "registry-pvc"
      capacity: "50"
      access_mode: "ReadWriteMany"
      
  - name: Check for registry PVC
    command: oc get pvc "{{ claim_name }}"
    register: registry_pvc_out
    ignore_errors: true

  - name: Create a YAML file for the PVC for the Registry volume
    template:
      src: templates/pvc.yaml.j2
      dest: /root/registry-pvc.yaml

  - name: Create PV from YAML for registry EBS volume
    command: oc create -f /root/registry-pv.yaml
    when: registry_pv_out | failed

  - name: Create PVC from YAML for registry EBS volume
    command: oc create -f /root/registry-pvc.yaml
    when: registry_pvc_out | failed

  - name: Check if registry is still using empty directory
    command: oc volume dc/docker-registry
    register: registry_dc_out

  - name: Attach volume to registry DC
    command: >
      oc volume dc/docker-registry --add --overwrite -t persistentVolumeClaim
      --claim-name=registry-pvc --name=registry-storage
    #debug: msg="Should see this this time"
    when: "'empty directory' in registry_dc_out.stdout"

  - name: Check if fsGroup is set in registry DC
    shell: "oc get dc/docker-registry -o yaml | grep fsGroup"
    register: fsgroup_out
    ignore_errors: true

  - name: Determine default project supplemental group
    command: oc get project default -o json
    register: default_project_out 
    when: fsgroup_out | failed

  - name: Process the default project json into a fact
    set_fact:
      default_project_json: "{{ default_project_out.stdout | from_json }}"
    when: fsgroup_out | failed

  - name: Patch the docker registry DC with the fsGroup
    command: oc patch dc/docker-registry -p '{"spec":{"template":{"spec":{"securityContext":{"fsGroup":{{ default_project_json["metadata"]["annotations"]["openshift.io/sa.scc.supplemental-groups"].split("/").0 }}}}}}}'
    when: fsgroup_out | failed

- name: Installation and Configuration of Log Aggregation
  hosts: project_master
  gather_facts: no
  vars_files:
  - vars.yml
  vars:
    default_context: 'default/openshift-internal-{{ r53_host_zone  | regex_replace("\.", "-") }}:{{ api_port }}/system:admin'
  tasks:

  # make sure that we are using the default user (system:admin) and the default project
  - name: Change the oc context
    command: "oc config use-context {{ default_context }}"

  - name: Check for the logging project
    command: "oc get project logging"
    register: logging_out
    ignore_errors: true
  
  # we will eventually want to look at the logging and metrics project, so
  # this is useful
  - name: Make admin user a cluster-admin
    command: oadm policy add-cluster-role-to-user cluster-admin admin

  # eventually we will change the region to be appropriate and this command will need to change
  - name: Create the logging project
    command: "oadm new-project logging"
    when: logging_out | failed

  - name: Remove the default node selector on the logging project
    command: oc patch namespace/logging -p '{"metadata":{"annotations":{"openshift.io/node-selector":""}}}'
    
  - name: Switch to the logging project
    command: "oc project logging"

  - name: Check for logging-deployer secret
    command: "oc get secret logging-deployer"
    register: logging_deployer_secret_out
    ignore_errors: true

  - name: Create the null logging-deployer secret
    command: oc secrets new logging-deployer nothing=/dev/null
    when: logging_deployer_secret_out | failed

  - name: Check for logging-deployer service account
    command: oc get sa logging-deployer
    register: logging_deployer_sa_out
    ignore_errors: true

  - name: Create the logging-deployer service account
    shell: 'echo ''{"apiVersion":"v1","kind":"ServiceAccount","metadata":{"name":"logging-deployer"},"secrets":[{"name":"logging-deployer"}]}'' | oc create -f -'
    when: logging_deployer_sa_out | failed

  - name: Wait for the logging-deployer secrets
    shell: "oc get secrets | grep logging-deployer-token | wc -l"
    register: deployer_token_out
    until: deployer_token_out.stdout | search("2")
    retries: 15
    delay: 10

  - name: Grant the edit role to the logging-deployer service account
    command: oc policy add-role-to-user edit system:serviceaccount:logging:logging-deployer

  - name: Put the fluentd service account in the privileged SCC
    command: oadm policy add-scc-to-user privileged system:serviceaccount:logging:aggregated-logging-fluentd

  - name: Give fluentd cluster-reader permissions
    command: oadm policy add-cluster-role-to-user cluster-reader system:serviceaccount:logging:aggregated-logging-fluentd

  # if the artifacts are already deployed, don't process the deployer template
  - name: Check for the deployed artifacts
    command: oc get template logging-support-template
    register: logging_support_template_out
    ignore_errors: true

  - name: Instantiate the logging deployer via the template
    shell: >
      oc process logging-deployer-template -n openshift 
      -v KIBANA_HOSTNAME=kibana."{{ r53_wildcard_zone }}" 
      -v PUBLIC_MASTER_URL=https://openshift."{{ r53_host_zone }}" 
      -v ES_CLUSTER_SIZE=1 
      -v ES_INSTANCE_RAM=1024M | oc create -f -
    when: logging_support_template_out | failed

  - name: Wait for the deployer to finish
    shell: "oc get pod | grep logging-deployer"
    register: deployer_output
    until: deployer_output.stdout | search("logging-deployer.*Completed.*")
    retries: 15
    delay: 10

  - name: Determine elastic search DC
    shell: "oc get dc | awk '/logging-es-[a-zA-Z0-9]*/{ print $1 }'"
    register: logging_es_out

  - name: Modify the kibana DC with a node selector for infra
    command: oc patch dc/logging-kibana -p '{"spec":{"template":{"spec":{"nodeSelector":{"env":"infra"}}}}}'

  - name: Modify the es DC with a node selector for infra
    command: oc patch dc/{{ logging_es_out.stdout }} -p '{"spec":{"template":{"spec":{"nodeSelector":{"env":"infra"}}}}}'

  # if the image streams exist, don't process the support template
  - name: Check for logging-kibana imagestream
    command: oc get is logging-kibana
    register: kibana_is_out
    ignore_errors: true

  - name: Process the logging support template
    shell: "oc process logging-support-template | oc create -f -"
    when: kibana_is_out | failed

  # we use a template to then lay down YAML to create the PV
  # this sets facts that are then consumed in the template
  - name: Set the facts for the logging PV template
    set_fact:
      pv_name: "logging-pv"
      capacity: "100"
      volid: "{{ hostvars['localhost']['logging_volume'] }}"
      access_mode: "ReadWriteMany"
      
  - name: Create a YAML file for the PV for the logging volume
    template:
      src: templates/pv.yaml.j2
      dest: /root/logging-pv.yaml

  - name: Check for logging PV
    command: oc get pv "{{ pv_name }}"
    register: logging_pv_out
    ignore_errors: true

  # as before
  - name: Set the facts for the logging PVC template
    set_fact:
      claim_name: "logging-pvc"
      capacity: "100"
      access_mode: "ReadWriteMany"
      
  - name: Check for logging PVC
    command: oc get pvc "{{ claim_name }}"
    register: logging_pvc_out
    ignore_errors: true

  - name: Create a YAML file for the PVC for the logging volume
    template:
      src: templates/pvc.yaml.j2
      dest: /root/logging-pvc.yaml

  - name: Create PV from YAML for logging EBS volume
    command: oc create -f /root/logging-pv.yaml
    when: logging_pv_out | failed

  - name: Create PVC from YAML for logging EBS volume
    command: oc create -f /root/logging-pvc.yaml
    when: logging_pvc_out | failed

  - name: Check if es is still using empty directory
    command: "oc volume dc/{{ logging_es_out.stdout }}"
    register: logging_dc_out

  - name: Attach volume to es DC
    command: >
      oc volume dc/{{ logging_es_out.stdout }} --add --overwrite -t persistentVolumeClaim
      --claim-name=logging-pvc --name=elasticsearch-storage
    when: "'empty directory' in logging_dc_out.stdout"

  - name: Check if fsGroup is set in logging DC
    shell: "oc get dc/{{ logging_es_out.stdout }} -o yaml | grep fsGroup"
    register: fsgroup_out
    ignore_errors: true

  - name: Determine logging project supplemental group
    command: oc get project logging -o json
    register: logging_project_out 
    when: fsgroup_out | failed

  - name: Process the logging project json into a fact
    set_fact:
      logging_project_json: "{{ logging_project_out.stdout | from_json }}"
    when: fsgroup_out | failed

  - name: Patch the es DC with the fsGroup
    command: oc patch dc/{{ logging_es_out.stdout }} -p '{"spec":{"template":{"spec":{"securityContext":{"fsGroup":{{ logging_project_json["metadata"]["annotations"]["openshift.io/sa.scc.supplemental-groups"].split("/").0 }}}}}}}'
    when: fsgroup_out | failed

  - name: Scale fluentd to number of nodes
    command: oc scale dc/logging-fluentd --replicas={{ groups['tag_openshift-demo-' ~ cluster_id ~ '-host-type_node'] | count }}

- name: Demonstration project configuration
  hosts: project_master
  vars:
    default_context: 'default/openshift-internal-{{ r53_host_zone  | regex_replace("\.", "-") }}:{{ api_port }}/system:admin'
  vars_files:
  - vars.yml
  tasks:
  - name: Find current projects list
    command: oc get projects
    register: projects

  - name: Create projects for internal users
    command: "oadm new-project {{ item.project }} --display-name='{{ item.project.title() }}' --node-selector='region={{ item.project }}' --admin='{{ item.user }}'"
    when: item.project not in projects.stdout and item.project != "empty"
    with_items: users

  - name: Switch to default project
    command: oc project default

  - name: Retrieve hexboard deployment configurations
    command: oc get dc/{{ hexboard.name }} -n {{ hexboard.namespace }}
    register: dcs_out
    ignore_errors: true

  - name: Login as the demo user
    command: oc login -u {{ users.0.user }} -p {{ default_password }} --certificate-authority=/etc/origin/master/ca.crt
    when: dcs_out | failed

  - name: "Get the demo user's token"
    script: files/get_token.sh
    register: auth_token
    when: dcs_out | failed

  - name: Set the token as a fact
    set_fact:
      access_token: "{{ auth_token.stdout }}"
    when: dcs_out | failed

  - name: Switch to the hexboard project
    command: oc project {{ hexboard.namespace }}
    when: dcs_out | failed

  - name: Install the hexboard template file on the master
    template:
      dest: /root/hexboard_template.json
      src: templates/hexboard_template.json.j2
    when: dcs_out | failed

  - name: Create the objects in the hexboard template
    command: oc create -f /root/hexboard_template.json
    when: dcs_out | failed
    ignore_errors: true

  - name: Start the hexboard build
    command: oc start-build {{ hexboard.name }}
    when: dcs_out | failed

  - name: Change the oc context
    command: "oc config use-context {{ default_context }}"
    when: dcs_out | failed
