# vim: set ft=ansible:
---
# to get the storage volume info and set up groups in case this playbook is run by itself
- include: cloudformation_setup.yml
- include: group_setup.yml

- name: Node post configuration
  hosts: nodes
  vars_files:
  - vars.yml
  tasks:
  - name: pre-pull images
    command: "docker pull {{ item }}"
    with_items: preload_images
    when: not prerelease | bool

- name: User creation
  hosts: masters
  vars:
    default_context: 'default/openshift-internal-{{ r53_host_zone  | regex_replace("\.", "-") }}:{{ api_port }}/system:admin'
  vars_files:
  - vars.yml
  tasks:
  - name: Change the oc context
    command: "oc config use-context {{ default_context }}"

  - name: Switch to default project
    command: oc project default

  - name: Create the default users
    command: "htpasswd -b /etc/origin/master/htpasswd {{ item.user }} {{ default_password }}"
    with_items: users

- name: Configuration of Router
  hosts: project_master
  vars_files:
  - vars.yml
  vars:
    default_context: 'default/openshift-internal-{{ r53_host_zone  | regex_replace("\.", "-") }}:{{ api_port }}/system:admin'
  tasks:
  - name: Change the oc context
    command: "oc config use-context {{ default_context }}"

  - name: Switch to default project
    command: oc project default

  - name: Scale router
    command: "oc scale --replicas={{ num_infra_nodes }} dc router"

  - name: Wait for scaled router
    script: files/router_scale.sh
    register: router_scale_out
    until: router_scale_out | success
    retries: 10
    delay: 15

# ** NOTE: Still using this because there's some weird bug when calling
# openshift-ansible from demo-ansible
# Using EBS storage with OpenShift requires that the systems
# know the EC2 credentials in order to manipulate the EBS volumes.
# masters require the keys in /etc/sysconfig/atomic-openshift-master
- name: Set up master EC2 credentials
  hosts: masters
  gather_facts: no
  vars_files:
  - vars.yml
  tasks:
  - name: Write EC2 key ID to /etc/sysconfig/atomic-openshift-master
    lineinfile:
      dest: /etc/sysconfig/atomic-openshift-master
      insertafter: EOF
      line: "AWS_ACCESS_KEY_ID={{ lookup('env','AWS_ACCESS_KEY_ID') }}"
      regexp: '^AWS_ACCESS_KEY_ID=.*'
    register: master_id_result

  - name: Write EC2 secret key to /etc/sysconfig/atomic-openshift-master
    lineinfile:
      dest: /etc/sysconfig/atomic-openshift-master
      insertafter: EOF
      line: "AWS_SECRET_ACCESS_KEY={{ lookup('env','AWS_SECRET_ACCESS_KEY') }}"
      regexp: '^AWS_SECRET_ACCESS_KEY=.*'
    register: master_key_result

  - name: Restart atomic-openshift-master-controllers service
    service:
      name: atomic-openshift-master-controllers
      state: restarted
    when: (master_id_result | changed) or (master_key_result | changed)

# Using EBS storage with OpenShift requires that the systems
# know the EC2 credentials in order to manipulate the EBS volumes.
# nodes require the keys in /etc/sysconfig/atomic-openshift-node
- name: Set up node EC2 credentials
  hosts: nodes
  gather_facts: no
  vars_files:
  - vars.yml
  tasks:
  - name: Write EC2 key ID to /etc/sysconfig/atomic-openshift-node
    lineinfile:
      dest: /etc/sysconfig/atomic-openshift-node
      insertafter: EOF
      line: "AWS_ACCESS_KEY_ID={{ lookup('env','AWS_ACCESS_KEY_ID') }}"
      regexp: '^AWS_ACCESS_KEY_ID=.*'
    register: node_id_result

  - name: Write EC2 secret key to /etc/sysconfig/atomic-openshift-node
    lineinfile:
      dest: /etc/sysconfig/atomic-openshift-node
      insertafter: EOF
      line: "AWS_SECRET_ACCESS_KEY={{ lookup('env','AWS_SECRET_ACCESS_KEY') }}"
      regexp: '^AWS_SECRET_ACCESS_KEY=.*'
    register: node_key_result

  - name: Restart atomic-openshift-node service
    service:
      name: atomic-openshift-node
      state: restarted
    when: (node_id_result | changed) or (node_key_result | changed)

- name: Installation and Configuration of Registry
  hosts: project_master
  gather_facts: no
  vars_files:
  - vars.yml
  vars:
    default_context: 'default/openshift-internal-{{ r53_host_zone  | regex_replace("\.", "-") }}:{{ api_port }}/system:admin'
  tasks:
    
    # make sure that we are using the default user (system:admin) and the default project
    - name: Change the oc context
      command: "oc config use-context {{ default_context }}"
    
    - name: Switch to default project
      command: oc project default
    
    # we use a template to then lay down YAML to create the PV
    # this sets facts that are then consumed in the template
    - name: Set the facts for the registry PV template
      set_fact:
        pv_name: "registry-pv"
        capacity: "50"
        volid: "{{ hostvars['localhost']['registry_volume'] }}"
    
    - name: Create a YAML file for the PV for the Registry volume
      template:
        src: templates/pv.yaml.j2
        dest: /root/registry-pv.yaml
    
    - name: Check for registry PV
      command: oc get pv "{{ pv_name }}"
      register: registry_pv_out
      ignore_errors: true
    
    # as before
    - name: Set the facts for the registry PVC template
      set_fact:
        claim_name: "registry-pvc"
        capacity: "50"
        access_mode: "ReadWriteMany"
    
    - name: Check for registry PVC
      command: oc get pvc "{{ claim_name }}"
      register: registry_pvc_out
      ignore_errors: true
    
    - name: Create a YAML file for the PVC for the Registry volume
      template:
        src: templates/pvc.yaml.j2
        dest: /root/registry-pvc.yaml
    
    - name: Create PV from YAML for registry EBS volume
      command: oc create -f /root/registry-pv.yaml
      when: registry_pv_out | failed
    
    - name: Create PVC from YAML for registry EBS volume
      command: oc create -f /root/registry-pvc.yaml
      when: registry_pvc_out | failed
    
    - name: Disable config change trigger on registry DC
      command: oc patch dc/docker-registry -p '{"spec":{"triggers":[]}}'
    
    - name: Check if registry is still using empty directory
      command: oc volume dc/docker-registry
      register: registry_dc_out
        
    - name: Attach volume to registry DC
      command: >
        oc volume dc/docker-registry --add --overwrite -t persistentVolumeClaim
        --claim-name=registry-pvc --name=registry-storage
      when: "'empty directory' in registry_dc_out.stdout"
      register: registry_volume_attached
    
    - name: Check if fsGroup is set in registry DC
      shell: "oc get dc/docker-registry -o yaml | grep fsGroup"
      register: fsgroup_out
      ignore_errors: true
    
    - name: Determine default project supplemental group
      command: oc get project default -o json
      register: default_project_out
      when: fsgroup_out | failed
    
    - name: Process the default project json into a fact
      set_fact:
        default_project_json: "{{ default_project_out.stdout | from_json }}"
      when: fsgroup_out | failed
    
    - name: Patch the docker registry DC with the fsGroup
      command: oc patch dc/docker-registry -p '{"spec":{"template":{"spec":{"securityContext":{"fsGroup":{{ default_project_json["metadata"]["annotations"]["openshift.io/sa.scc.supplemental-groups"].split("/").0 }}}}}}}'
      when: fsgroup_out | failed
    
    - name: Cancel any current deployments
      command: oc deploy docker-registry --cancel
      when: fsgroup_out | failed or registry_volume_attached | success
    
    - name: Deploy latest configuration of registry DC
      command: oc deploy docker-registry --latest
      when: fsgroup_out | failed or registry_volume_attached | success
      register: deploy_latest
    
    - name: Re-enable config trigger on docker-registry
      command: oc patch dc/docker-registry -p '{"spec":{"triggers":[{"type":"ConfigChange"}]}}'
      when: deploy_latest | success
    
    - name: Check if registry is running
      script: files/check_registry_running.sh
      register: registry_running
      until: registry_running | success
      retries: 10
      delay: 15
    
- name: Installation and Configuration of Log Aggregation
  hosts: project_master
  gather_facts: no
  vars_files:
  - vars.yml
  vars:
    default_context: 'default/openshift-internal-{{ r53_host_zone  | regex_replace("\.", "-") }}:{{ api_port }}/system:admin'
    image_prefix: "{{ registry_url | regex_replace('^(.*\\/).*$', '\\\\1') }}"
    registry_fqdn: "{{ registry_url | regex_replace('(.*?)\\/.*$', '\\\\1') }}"
  tasks:

  # make sure that we are using the default user (system:admin) and the default project
  - name: Change the oc context
    command: "oc config use-context {{ default_context }}"

  - name: Check for the logging project
    command: "oc get project logging"
    register: logging_out
    ignore_errors: true

  # we will eventually want to look at the logging and metrics project, so
  # this is useful
  - name: Make admin user a cluster-admin
    command: oadm policy add-cluster-role-to-user cluster-admin admin

  # eventually we will change the region to be appropriate and this command will need to change
  - name: Create the logging project
    command: "oadm new-project logging"
    when: logging_out | failed

  - name: Remove the default node selector on the logging project
    command: oc patch namespace/logging -p '{"metadata":{"annotations":{"openshift.io/node-selector":""}}}'

  - name: Switch to the logging project
    command: "oc project logging"

  # for prerelease / QE stuff
  - name: Delete Docker auth secret if it exists
    command: oc delete secret prerelease
    ignore_errors: true
    when: prerelease | bool

  - name: Create the Docker auth secret
    command: oc secrets new-dockercfg prerelease --docker-server={{ registry_fqdn }} --docker-username={{ kerberos_user }}@redhat.com --docker-password={{ kerberos_token }} --docker-email={{ kerberos_user }}@redhat.com
    when: prerelease | bool

  - name: Check for logging-deployer secret
    command: "oc get secret logging-deployer"
    register: logging_deployer_secret_out
    ignore_errors: true

  - name: Create the null logging-deployer secret
    command: oc secrets new logging-deployer nothing=/dev/null
    when: logging_deployer_secret_out | failed

  - name: Check for logging-deployer service account
    command: oc get sa logging-deployer
    register: logging_deployer_sa_out
    ignore_errors: true

  - name: Create the logging-deployer service account
    shell: 'echo ''{"apiVersion":"v1","kind":"ServiceAccount","metadata":{"name":"logging-deployer"},"secrets":[{"name":"logging-deployer"}]}'' | oc create -f -'
    when: logging_deployer_sa_out | failed

  - name: Wait for the logging-deployer secrets
    shell: "oc get secrets | grep logging-deployer-token"
    register: deployer_token_out
    until: deployer_token_out | success
    retries: 15
    delay: 10

  - name: Grant the edit role to the logging-deployer service account
    command: oc policy add-role-to-user edit system:serviceaccount:logging:logging-deployer

  - name: Put the fluentd service account in the privileged SCC
    command: oadm policy add-scc-to-user privileged system:serviceaccount:logging:aggregated-logging-fluentd

  - name: Give fluentd cluster-reader permissions
    command: oadm policy add-cluster-role-to-user cluster-reader system:serviceaccount:logging:aggregated-logging-fluentd

  # if the artifacts are already deployed, don't process the deployer template
  - name: Check for the deployed artifacts
    command: oc get template logging-support-template
    register: logging_support_template_out
    ignore_errors: true

  - name: Instantiate the logging deployer via the template
    shell: >
      oc process logging-deployer-template -n openshift
      -v IMAGE_PREFIX="{{ image_prefix }}"
      -v KIBANA_HOSTNAME=kibana."{{ r53_wildcard_zone }}"
      -v PUBLIC_MASTER_URL=https://openshift."{{ r53_host_zone }}"
      -v ES_CLUSTER_SIZE=1
      -v ES_INSTANCE_RAM=1024M | oc create -f -
    when: logging_support_template_out | failed

  - name: Wait for the deployer to finish
    script: files/check_pod_complete.sh 'logging-deployer-[a-zA-Z0-9]*'
    register: check_out
    until: check_out | success
    retries: 15
    delay: 10

  - name: Determine elastic search DC
    shell: "oc get dc | awk '/logging-es-[a-zA-Z0-9]*/{ print $1 }'"
    register: logging_es_out
  
  - name: Modify the kibana DC with a node selector for infra
    command: oc patch dc/logging-kibana -p '{"spec":{"template":{"spec":{"nodeSelector":{"env":"infra"}}}}}'
  
  - name: Modify the es DC with a node selector for infra
    command: oc patch dc/{{ logging_es_out.stdout }} -p '{"spec":{"template":{"spec":{"nodeSelector":{"env":"infra"}}}}}'
  
  # if the image streams exist, don't process the support template
  - name: Check for logging-kibana imagestream
    command: oc get is logging-kibana
    register: kibana_is_out
    ignore_errors: true
  
  - name: Process the logging support template
    shell: "oc process logging-support-template | oc create -f -"
    when: kibana_is_out | failed
  
  - name: Determine image stream version tags
    script: files/image_stream_version_check.sh logging-support-template logging
    register: is_version_out

  - name: "Pull the image stream tags" 
    command: oc import-image {{ item }}:{{ is_version_out.stdout }} --insecure=true
    with_items:
      - "logging-auth-proxy"
      - "logging-elasticsearch"
      - "logging-fluentd"
      - "logging-kibana"
    register: image_tag_pull_out
    retries: 2
    until: image_tag_pull_out | success

  # we use a template to then lay down YAML to create the PV
  # this sets facts that are then consumed in the template
  - name: Set the facts for the logging PV template
    set_fact:
      pv_name: "logging-pv"
      capacity: "100"
      volid: "{{ hostvars['localhost']['logging_volume'] }}"
  
  - name: Create a YAML file for the PV for the logging volume
    template:
      src: templates/pv.yaml.j2
      dest: /root/logging-pv.yaml
  
  - name: Check for logging PV
    command: oc get pv "{{ pv_name }}"
    register: logging_pv_out
    ignore_errors: true
  
  # as before
  - name: Set the facts for the logging PVC template
    set_fact:
      claim_name: "logging-pvc"
      capacity: "100"
      access_mode: "ReadWriteMany"
  
  - name: Check for logging PVC
    command: oc get pvc "{{ claim_name }}"
    register: logging_pvc_out
    ignore_errors: true
  
  - name: Create a YAML file for the PVC for the logging volume
    template:
      src: templates/pvc.yaml.j2
      dest: /root/logging-pvc.yaml
  
  - name: Create PV from YAML for logging EBS volume
    command: oc create -f /root/logging-pv.yaml
    when: logging_pv_out | failed
  
  - name: Create PVC from YAML for logging EBS volume
    command: oc create -f /root/logging-pvc.yaml
    when: logging_pvc_out | failed
  
  - name: Check if es is still using empty directory
    command: "oc volume dc/{{ logging_es_out.stdout }}"
    register: logging_dc_out
  
  - name: Attach volume to es DC
    command: >
      oc volume dc/{{ logging_es_out.stdout }} --add --overwrite -t persistentVolumeClaim
      --claim-name=logging-pvc --name=elasticsearch-storage
    when: "'empty directory' in logging_dc_out.stdout"
  
  - name: Check if fsGroup is set in logging DC
    shell: "oc get dc/{{ logging_es_out.stdout }} -o yaml | grep fsGroup"
    register: fsgroup_out
    ignore_errors: true
  
  - name: Determine logging project supplemental group
    command: oc get project logging -o json
    register: logging_project_out
    when: fsgroup_out | failed
  
  - name: Process the logging project json into a fact
    set_fact:
      logging_project_json: "{{ logging_project_out.stdout | from_json }}"
    when: fsgroup_out | failed
  
  - name: Patch the es DC with the fsGroup
    command: oc patch dc/{{ logging_es_out.stdout }} -p '{"spec":{"template":{"spec":{"securityContext":{"fsGroup":{{ logging_project_json["metadata"]["annotations"]["openshift.io/sa.scc.supplemental-groups"].split("/").0 }}}}}}}'
    when: fsgroup_out | failed
  
  - name: Scale fluentd to number of nodes
    command: oc scale dc/logging-fluentd --replicas={{ groups['tag_openshift-demo-' ~ cluster_id ~ '-host-type_node'] | count }}

- name: Installation and Configuration of Metrics
  hosts: project_master
  gather_facts: no
  vars_files:
  - vars.yml
  vars:
    default_context: 'default/openshift-internal-{{ r53_host_zone  | regex_replace("\.", "-") }}:{{ api_port }}/system:admin'
    image_prefix: "{{ registry_url | regex_replace('^(.*\\/).*$', '\\\\1') }}"
    registry_fqdn: "{{ registry_url | regex_replace('(.*?)\\/.*$', '\\\\1') }}"
  tasks:

  # make sure that we are using the default user (system:admin) and the default project
  - name: Change the oc context
    command: "oc config use-context {{ default_context }}"

  - name: Change to the openshift-infra project
    command: "oc project openshift-infra"

  # for prerelease / QE stuff
  - name: Delete Docker auth secret if it exists
    command: oc delete secret prerelease
    ignore_errors: true
    when: prerelease | bool

  - name: Create the Docker auth secret
    command: oc secrets new-dockercfg prerelease --docker-server={{ registry_fqdn }} --docker-username={{ kerberos_user }}@redhat.com --docker-password={{ kerberos_token }} --docker-email={{ kerberos_user }}@redhat.com
    when: prerelease | bool

  - name: Force metrics components into env=infra
    command: oc patch namespace/openshift-infra -p '{"metadata":{"annotations":{"openshift.io/node-selector":"env=infra"}}}'

  - name: Check for metrics-deployer service account
    command: oc get sa metrics-deployer
    register: metrics_deployer_sa_out
    ignore_errors: true

  - name: Create the metrics-deployer service account
    shell: 'echo ''{"apiVersion":"v1","kind":"ServiceAccount","metadata":{"name":"metrics-deployer"},"secrets":[{"name":"metrics-deployer"}]}'' | oc create -f -'
    when: metrics_deployer_sa_out | failed

  - name: Wait for the metrics-deployer secrets
    shell: "oc get secrets | grep metrics-deployer-token"
    register: metrics_token_out
    until: metrics_token_out | success
    retries: 15
    delay: 10

  - name: Grant the edit role to the metrics-deployer service account
    command: oadm policy add-role-to-user edit system:serviceaccount:openshift-infra:metrics-deployer

  - name: Grant the cluster-reader role to the heapster service account
    command: oadm policy add-cluster-role-to-user cluster-reader system:serviceaccount:openshift-infra:heapster

  - name: Check for metrics-deployer secret
    command: "oc get secret metrics-deployer"
    register: metrics_deployer_secret_out
    ignore_errors: true

  - name: Create the null metrics-deployer secret
    command: oc secrets new metrics-deployer nothing=/dev/null
    when: metrics_deployer_secret_out | failed

  # we use a template to then lay down YAML to create the PV
  # this sets facts that are then consumed in the template
  - name: Set the facts for the metrics PV template
    set_fact:
      pv_name: "metrics-pv"
      capacity: "100"
      volid: "{{ hostvars['localhost']['metrics_volume'] }}"

  - name: Create a YAML file for the PV for the metrics volume
    template:
      src: templates/pv.yaml.j2
      dest: /root/metrics-pv.yaml

  - name: Check for metrics PV
    command: oc get pv "{{ pv_name }}"
    register: metrics_pv_out
    ignore_errors: true

  - name: Create PV from YAML for metrics EBS volume
    command: oc create -f /root/metrics-pv.yaml
    when: metrics_pv_out | failed

  # if the artifacts are already deployed, don't process the deployer template
  - name: Check for the deployed artifacts
    command: oc get rc hawkular-metrics
    register: metrics_artifacts_out
    ignore_errors: true

  - name: Instantiate the logging deployer via the template
    shell: >
      oc process metrics-deployer-template -n openshift
      -v IMAGE_PREFIX="{{ image_prefix }}"
      -v CASSANDRA_PV_SIZE=100Gi
      -v HAWKULAR_METRICS_HOSTNAME=metrics."{{ r53_wildcard_zone }}" | oc create -f -
    when: metrics_artifacts_out | failed

  - name: Wait for the deployer to finish
    script: files/check_pod_complete.sh 'metrics-deployer-[a-zA-Z0-9]*'
    register: check_out
    until: check_out | success
    retries: 15
    delay: 10
  
  - name: Wait for the hawkular-cassandra-1 RC to exist
    command: oc get rc hawkular-cassandra-1
    register: rc_out
    until: rc_out.stdout | search("hawkular-cassandra-1")
    retries: 15
    delay: 10
  
  - name: Check if fsGroup is set in cassandra RC
    shell: "oc get rc/hawkular-cassandra-1 -o yaml | grep fsGroup"
    register: fsgroup_out
    ignore_errors: true
  
  - name: Determine openshift-infra project supplemental group
    command: oc get project openshift-infra -o json
    register: infra_project_out
    when: fsgroup_out | failed
  
  - name: Process the openshift-infra project json into a fact
    set_fact:
      infra_project_json: "{{ infra_project_out.stdout | from_json }}"
    when: fsgroup_out | failed
  
  - name: Patch the cassandra RC with the fsGroup
    command: oc patch rc/hawkular-cassandra-1 -p '{"spec":{"template":{"spec":{"securityContext":{"fsGroup":{{ infra_project_json["metadata"]["annotations"]["openshift.io/sa.scc.supplemental-groups"].split("/").0 }}}}}}}'
    when: fsgroup_out | failed
    register: patched_out
  
  - name: Find the cassandra pod
    shell: oc get pod | awk '/hawkular-cassandra-1/{ print $1 }'
    register: cassandra_pod_out
    when: patched_out | success
  
  - name: Delete the cassandra pod to get the fsGroup into it
    command: "oc delete pod {{ cassandra_pod_out.stdout }}"
    when: patched_out | success

  - name: Check if the stats resolution has been set
    shell: oc get rc/heapster -o json | grep resolution
    register: resolution_out
    ignore_errors: true

  - name: Patch the heapster RC
    command: oc patch rc/heapster -p '{"spec":{"template":{"spec":{"containers":[{"name":"heapster","image":"registry.access.redhat.com/openshift3/metrics-heapster:3.2.0","command":["heapster-wrapper.sh","--wrapper.username_file=/hawkular-account/hawkular-metrics.username","--wrapper.password_file=/hawkular-account/hawkular-metrics.password","--wrapper.allowed_users_file=/secrets/heapster.allowed-users","--wrapper.endpoint_check=https://hawkular-metrics:443/hawkular/metrics/status","--source=kubernetes:https://kubernetes.default.svc:443?useServiceAccount=true&kubeletHttps=true&kubeletPort=10250","--sink=hawkular:https://hawkular-metrics:443?tenant=_system&labelToTenant=pod_namespace&labelNodeId=nodename&caCert=/hawkular-cert/hawkular-metrics-ca.certificate&user=%username%&pass=%password%&filter=label(container_name:^/system.slice.*|^/user.slice)","--tls_cert=/secrets/heapster.cert","--tls_key=/secrets/heapster.key","--tls_client_ca=/secrets/heapster.client-ca","--allowed_users=%allowed_users%","--stats_resolution=30s"],"ports":[{"name":"http-endpoint","containerPort":8082,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"heapster-secrets","mountPath":"/secrets"},{"name":"hawkular-metrics-certificate","mountPath":"/hawkular-cert"},{"name":"hawkular-metrics-account","mountPath":"/hawkular-account"}],"readinessProbe":{"exec":{"command":["/opt/heapster-readiness.sh"]},"timeoutSeconds":1,"periodSeconds":10,"successThreshold":1,"failureThreshold":3},"terminationMessagePath":"/dev/termination-log","imagePullPolicy":"IfNotPresent"}]}}}}'
    when: resolution_out | failed

  - name: Find the heapster pod
    shell: oc get pod | awk '/heapster/{ print $1 }'
    register: heapster_pod_out

  - name: Kill the heapster pod
    shell: oc delete pod {{ heapster_pod_out.stdout }}

  - name: Wait for old heapster pod to be gone
    shell: oc get pod | grep {{ heapster_pod_out.stdout }}
    register: metrics_pods_out
    until: metrics_pods_out | failed
    retries: 15
    delay: 10
    ignore_errors: true

  - name: Wait for new heapster pod to be running
    shell: oc get pod | grep heapster | grep -i unning
    register: heapster_running_out
    until: heapster_running_out | success
    retries: 15
    delay: 10
    ignore_errors: true

- name: Demonstration project configuration
  hosts: project_master
  vars:
    default_context: 'default/openshift-internal-{{ r53_host_zone  | regex_replace("\.", "-") }}:{{ api_port }}/system:admin'
  vars_files:
  - vars.yml
  tasks:
  - name: Find current projects list
    command: oc get projects
    register: projects

  - name: Create projects for internal users
    command: "oadm new-project {{ item.project }} --display-name='{{ item.project.title() }}' --admin='{{ item.user }}'"
    when: item.project not in projects.stdout and item.project != "empty"
    with_items: users

  - name: Switch to default project
    command: oc project default

  - name: Retrieve hexboard deployment configurations
    command: oc get dc/{{ hexboard.name }} -n {{ hexboard.namespace }}
    register: dcs_out
    ignore_errors: true

  - name: Login as the demo user
    command: oc login -u {{ users.0.user }} -p {{ default_password }} --certificate-authority=/etc/origin/master/ca.crt
    when: dcs_out | failed

  - name: "Get the demo user's token"
    script: files/get_token.sh
    register: auth_token
    when: dcs_out | failed

  - name: Set the token as a fact
    set_fact:
      access_token: "{{ auth_token.stdout }}"
    when: dcs_out | failed

  - name: Switch to the hexboard project
    command: oc project {{ hexboard.namespace }}
    when: dcs_out | failed

  - name: Install the hexboard template file on the master
    template:
      dest: /root/hexboard_template.json
      src: templates/hexboard_template.json.j2
    when: dcs_out | failed

  - name: Create the objects in the hexboard template
    command: oc create -f /root/hexboard_template.json
    when: dcs_out | failed
    ignore_errors: true

  - name: Start the hexboard build
    command: oc start-build {{ hexboard.name }}
    when: dcs_out | failed

  - name: Change the oc context
    command: "oc config use-context {{ default_context }}"
    when: dcs_out | failed
